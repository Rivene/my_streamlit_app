# # main.py
# import streamlit as st

# st.text('hello Streamlit!')

# youtube_trend_conversation.py
import streamlit as st
import requests
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import pandas as pd
import re
from collections import Counter
# :ì—´ì‡ ì™€_ì ê¸´_ìë¬¼ì‡ : ìœ íŠœë¸Œ API í‚¤ (â† ë³¸ì¸ì˜ í‚¤ë¡œ ë³€ê²½)
API_KEY = "AIzaSyCHDXTtIcfegyjBplVooLoeBud1dkchfGA"
# :ì•ìª½_í™”ì‚´í‘œ: ì±„ë„ ID ì…ë ¥ ë°›ê¸°
st.set_page_config(page_title="YouTube íŠ¸ë Œë“œ & ëŒ€í™” ì£¼ì œ ë¶„ì„ê¸°", page_icon=":í´ë˜í¼:", layout="wide")
st.title("ğŸ¬: ìœ íŠœë¸Œ ì¸ê¸° íŠ¸ë Œë“œ ë¶„ì„ & ëŒ€í™” ì£¼ì œ ì¶”ì²œ")
st.write("ìœ íŠœë¸Œ ì±„ë„ì˜ ì¸ê¸° ì˜ìƒì„ ë¶„ì„í•´ í‚¤ì›Œë“œì™€ ëŒ€í™” ì£¼ì œë¥¼ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤.")
channel_id = st.text_input("ğŸ“º: ìœ íŠœë¸Œ ì±„ë„ ID", help="ì˜ˆ: UC_x5XG1OV2P6uZZ5FSM9Ttw")
video_count = st.slider("ğŸ: ë¶„ì„í•  ì˜ìƒ ê°œìˆ˜", 5, 20, 10)
# :ë¹—ìë£¨: í…ìŠ¤íŠ¸ ì •ì œ

def clean_text(text):
    text = re.sub(r"[^\uAC00-\uD7A3a-zA-Z0-9\s]", "", text)
    return text.lower()

# :ëŒ€ë¬¸ì_abcd: í‚¤ì›Œë“œ ì¶”ì¶œ (í•œê¸€ 2ê¸€ì ì´ìƒë§Œ)
def extract_keywords(texts, top_n=30):
    all_words = []
    for text in texts:
        # í•œê¸€ 2ê¸€ì ì´ìƒë§Œ ì¶”ì¶œ
        words = re.findall(r'[ê°€-í£]{2,}', clean_text(text))
        all_words.extend(words)
    return Counter(all_words).most_common(top_n)

# :êµ¬ë¦„: ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± (í•œê¸€ í°íŠ¸ ì§€ì •)
def create_wordcloud(freq_dict):
    wc = WordCloud(font_path="nanumgothic.ttf", background_color="white", width=800, height=400)
    wc.generate_from_frequencies(freq_dict)
    return wc
# :ì‹œê³„_ë°˜ëŒ€_ë°©í–¥_í™”ì‚´í‘œ: ì±„ë„ â†’ ì—…ë¡œë“œ ë¦¬ìŠ¤íŠ¸ ID
def get_upload_playlist_id(channel_id):
    url = f"https://www.googleapis.com/youtube/v3/channels?part=contentDetails&id={channel_id}&key={API_KEY}"
    res = requests.get(url).json()
    try:
        return res['items'][0]['contentDetails']['relatedPlaylists']['uploads']
    except:
        return None
# :ë°›ì€_í¸ì§€í•¨_íŠ¸ë ˆì´: ì—…ë¡œë“œ ë¦¬ìŠ¤íŠ¸ â†’ ì˜ìƒ ì •ë³´
def get_videos_from_playlist(playlist_id, max_results):
    url = f"https://www.googleapis.com/youtube/v3/playlistItems?part=snippet&playlistId={playlist_id}&maxResults={max_results}&key={API_KEY}"
    res = requests.get(url).json()
    videos = []
    for item in res.get("items", []):
        snippet = item["snippet"]
        videos.append({
            "video_id": snippet["resourceId"]["videoId"],
            "title": snippet["title"],
            "description": snippet["description"],
            "url": f"https://www.youtube.com/watch?v={snippet['resourceId']['videoId']}"
        })
    return videos
# :ë§í’ì„ : ëŒ€í™” ì£¼ì œ ìƒì„±
def generate_conversation_topics(keywords):
    prompts = []
    for keyword, _ in keywords[:10]:
        if "ê²Œì„" in keyword:
            prompts.append(f"ğŸ®: ìš”ì¦˜ '{keyword}' ê´€ë ¨ ì½˜í…ì¸  ë§ì´ ë³´ì´ë˜ë°, ì¦ê²¨ í•˜ì„¸ìš”?")
        elif "ì—¬í–‰" in keyword or "íœ´ê°€" in keyword:
            prompts.append(f"âœˆï¸: '{keyword}' ê´€ë ¨ ì˜ìƒì´ ì¸ê¸°ì¸ë°, ìµœê·¼ì— ì–´ë”” ë‹¤ë…€ì˜¤ì…¨ì–´ìš”?")
        elif "ìŒì•…" in keyword or "ë…¸ë˜" in keyword:
            prompts.append(f"ğŸµ: '{keyword}' ì˜ìƒì´ ë§ì€ë°, ì–´ë–¤ ìŒì•… ì¢‹ì•„í•˜ì„¸ìš”?")
        elif "ë‹¤ì´ì–´íŠ¸" in keyword or "í—¬ìŠ¤" in keyword:
            prompts.append(f"ğŸ’ª: '{keyword}' ê´€ë ¨í•œ ì˜ìƒì´ ë§ë„¤ìš”. ê±´ê°•ê´€ë¦¬ ì–´ë–»ê²Œ í•˜ì„¸ìš”?")
        elif "ë“œë¼ë§ˆ" in keyword or "ì˜í™”" in keyword:
            prompts.append(f"ğŸ¬: '{keyword}' ì˜ìƒì´ í•«í•œë°, í˜¹ì‹œ ìµœê·¼ ë³¸ ê±° ìˆìœ¼ì„¸ìš”?")
        else:
            prompts.append(f"ğŸ—£ï¸: '{keyword}' ìš”ì¦˜ í•«í•œ ì£¼ì œ ê°™ì•„ìš”. ì–´ë–»ê²Œ ìƒê°í•˜ì„¸ìš”?")
    return prompts[:5]
# :ë¡œì¼“: ë¶„ì„ ì‹œì‘
if st.button(": íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘"):
    if not channel_id:
        st.error("ì±„ë„ IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner("YouTube ì±„ë„ ì •ë³´ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
            playlist_id = get_upload_playlist_id(channel_id)
        if not playlist_id:
            st.error("ì±„ë„ IDê°€ ì˜ëª»ë˜ì—ˆê±°ë‚˜ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            with st.spinner("ì¸ê¸° ì˜ìƒ ë¶„ì„ ì¤‘..."):
                videos = get_videos_from_playlist(playlist_id, video_count)
            if not videos:
                st.warning("ì˜ìƒì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                titles = [v["title"] for v in videos]
                descriptions = [v["description"] for v in videos]
                texts = titles + descriptions
                keywords = extract_keywords(texts)
                keyword_df = pd.DataFrame(keywords)
                keyword_df.columns = ["í‚¤ì›Œë“œ", "ë¹ˆë„"]
                # :êµ¬ë¦„: ì›Œë“œí´ë¼ìš°ë“œ
                st.subheader("â˜ï¸: ì›Œë“œí´ë¼ìš°ë“œ")
                fig, ax = plt.subplots(figsize=(10, 5))
                wc = create_wordcloud(dict(keywords))
                ax.imshow(wc, interpolation="bilinear")
                ax.axis("off")
                st.pyplot(fig)
                # :ë§‰ëŒ€_ì°¨íŠ¸: í‚¤ì›Œë“œ ë°” ì°¨íŠ¸
                st.subheader("ğŸ“Š: í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜")
                st.bar_chart(keyword_df.set_index("í‚¤ì›Œë“œ"))
                # :ë§í’ì„ : ëŒ€í™” ì£¼ì œ ì¶”ì²œ
                st.subheader("ğŸ’¬: ì¶”ì²œ ëŒ€í™” ì£¼ì œ")
                topics = generate_conversation_topics(keywords)
                for i, t in enumerate(topics, 1):
                    st.write(f"**{i}.** {t}")
                # :í•„ë¦„_í”„ë ˆì„: ì˜ìƒ ëª©ë¡
                st.subheader("ğŸ¥: ì¸ê¸° ì˜ìƒ ëª©ë¡")
                for v in videos:
                    st.markdown(f"- [{v['title']}]({v['url']})")
                # :ë°›ì€_í¸ì§€í•¨_íŠ¸ë ˆì´: ë‹¤ìš´ë¡œë“œ
                st.subheader("ğŸ“¥: ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")
                csv = keyword_df.to_csv(index=False)
                st.download_button("ğŸ“„:ê¸€ì”¨ê°€_ì“°ì—¬ì§„_í˜ì´ì§€: í‚¤ì›Œë“œ CSV ë‹¤ìš´ë¡œë“œ", csv, "youtube_trend_keywords.csv", mime="text/csv")
                txt = "\n".join(topics)
                st.download_button("ğŸ’¬: ëŒ€í™” ì£¼ì œ TXT ë‹¤ìš´ë¡œë“œ", txt, "youtube_conversation_topics.txt", mime="text/plain")